 Emotional Recognition Using a Compensation Transformation in Speech Signal

Abstract
An effective method based on GMM is proposed in this paper for speech emotional recognition; a compensation transformation is introduced in the recognition stage to reduce the influence of variations in speech characteristics and noise. The extraction of emotional features includes the globe feature, time series structure feature, LPCC, MFCC and PLP. Five human emotions (happiness, angry, surprise, sadness and neutral) are investigated. The result shows that it can increase the recognition ratio more than normal GMM; the method in this paper is effective and robust.
Key words: Speech Emotional Recognition (SER), GMM, Emotion Recognition, Compensation Transformation
1. Introduction
One of the natural goals for research on speech signals is recognizing emotions of humans [Chen 1987; Oppenheim 1976; Cowie 2001]; it has gained growing amounts of interest over the last 20 years. A study conducted by Shirasawa et al. showed that SER could be made by ICA and attain an 87% average recognition ratio [Shirasawa 1997; Shirasawa 1999] Many studies have been conducted to investigate neural networks for SER. Chang-Hyun Park tried to recognize sequentially inputted data using DRNN in 2003[Park et al. 2003], Muhammad, W. B. obtained about 79% recognition rate using GRNN [Bhatti et al. 2004]. Aishah Abdul Razak achieved an average recognition rate of 62.35% using combination MLP [Razak et al. 2005]. Fuzzy rules are also introduced into SER such that an 84% rate has been achieved in recognizing anger and sadness [Austermann et al. 2005]. A number of studies in SER have also been done with the development of GMM/HMM [Rabiner 1989; Jiang et al. 2004; Lin et al. 2005]. However, in SER, the variations in speech characteristics, noise and individual differences always influence the recognition results. In addition, the methods above have always handled such problems in the preprocessing stage and have not been able to eliminate the influence effectively. Therefore, a valid solution has still not been proposed. In this paper a compensation transformation is introduced into an algorithm for GMM which operates in the recognition module. The experiments with five emotions (happiness, angry, neutral, surprise and sadness) show that the method in this paper is effective in emotional recognition.
2. Descriptions of Emotion and Selection of Emotion Speech Materials
Usually, emotions are classified into two main categories: basic emotions and derived emotions. Basic emotions, generally, can be found in all mammals. Derived emotions mean derivations from basic emotions. One viewpoint is that the basic emotions are composed by the basic mood. Due to different research backgrounds, different researchers have expressed different definitions of basic emotions. Some of the major definitions [Ortony et al. 1990] of the basic emotions are shown in Table 1.
Table 1. Researches about basic emotions definition
  Researchers Plutchik
Ekman/Friesen/ Ellsworth
James Izard
Oatley/Johnson -Laird
Panksepp Weiner/Graham
definitions
Acceptance, joy, anger, anticipation, disgust, fear, sadness, surprise
Anger, disgust, fear, joy, sadness, surprise
Fear, grief, love, rage
Anger, contempt, disgust, distress, fear, guilt, interest, joy, shame, surprise
Anger, disgust, anxiety, happiness, sadness
Expectancy, fear, rage, panic Happiness, sadness
  The common emotion classification which was proposed by Plutchik is shown in Figure 1e. In this paper, the authors only recognize five kinds of emotion.
Joy
Disgust
Acceptance Fear
Neutral Surprise
Sadness
 Anticipation
Anger
Figure 1. Emotion wheel


This is a relatively conservative view of what emotion is so special attention has been paid to emotional dimension space theory. Three major dimensions (valence, arousal, and control) [Cowie 2001] are used to describe emotions.
a. Valence: The clearest common element of emotional states is that the person is materially influenced by feelings that are valenced, i.e., they are centrally concerned with positive or negative evaluations of people or things or events.
b. Arousal: It has been proven that emotional states involve dispositions to act in certain ways. A basic way of reflecting that theme turns out to be surprisingly useful. States are simply rated in terms of the associated activation level, i.e., the strength of the person’s disposition to take some action rather than none.
c. Control: Embodying in the initiative and the degree of control. For instance, contempt and fear are in different ends of the control dimension.
In this paper, two aspects have to be taken into consideration in the selection of emotional materials: 1. the sentence materials can’t have any emotional tendency; 2. the materials should relate to five kinds of emotions (happiness, angry, surprise, sadness, and neutral). All recordings were carried out in a large, soundproof room with no echo interference using a high quality microphone, a SONY DAT recorder and a PC164 audio card at a sampling rate of l2KHZ with 16-bit resolution. Six speakers (three male and three female) who are good at acting spoke the sentences with happiness, anger, surprise and sadness, expressing each emotion three times. At the same time, the researchers made the speakers speak each sentence three times in a neutral way. In this way, 2430 sentences for experiments were compiled.
3. Feature Extraction
The emotional features of speech signals are always represented as the change of speech rhythm [Shigenaga 1999; Muraka 1998]. For example, when a man is in a rage, his speech rate, volume and tone will all get higher. Some characteristics of phonemes can also reflect the change of emotions such as formant and the cross section of the vocal tract [Muraka 1998; Zhao et al. 2001]. As the emotional information of speech signals is more or less related to the meaning of the sentences, the distributing rules and construction characteristics should be attained by analyzing the relationship between emotional speech and neutral speech to avoid the effect caused by the meaning of the sentences.
The global features used in this paper are duration, mean pitch, maximum pitch, average different rate of pitch, average amplitude power, amplitude power dynamic range, average frequency of formant, average different rate of formant, mean slope of the regression line of the peak value of the formant and the average peak value of formant [Zhao et al. 2001; Zhao et al. 2000; Zhao et al. 2000]. The duration is the continuous time from start to end in each emotional sentence. It includes the silence, because these parts contribute to the emotion. Duration ratio of emotional speech and neutral speech was used as the characteristic parameters for recognition. The frequency of pitch was obtained by calculating cepstrum. Then the pitch-track was gained, and maximum pitch ( F0max ), average fundamental frequency ( F0), average different rate of pitch ( F0rate ) of the envelopes of different emotional speech signals can all be extracted from it. F0rate mentioned here, refers to the mean absolute value of the difference between each frame of speech signal ’s fundamental frequencies. The authors used the differences in value of the mean pitch, the maximum pitch and the ratio of F0rate between the emotional and neutral speech as the characteristic parameters. In this paper, the average amplitude power ( A ) and the dynamic range ( Arange ) are to be taken into account. To avoid the influence of the silent and noisy parts of the speech, the authors only took the mean absolute value of the amplitude into account and all the absolute values must above a threshold. The difference of average amplitude power and the dynamic range between the emotional and neutral speech was used for parameters of recognition. Formant is an important parameter that reflects the characteristics of vocal track. Formant was attained as follows [Zhao et al. 2001]. At first, LPC method was applied to calculate 14-order coefficients of linear prediction. Then, the coefficients were used to estimate the track’s frequency of the formant by analyzing the frequency average ( F1), frequency-changing rate ( F1rate ) of the first formant, the average and the average slope of recursive lines of the first four formants. The authors use the difference of F1 , the last two parameters and the ratio of F1rate between the emotional and neutral speech as the characters in each frame.
The structural features of time series for the emotional sentences used in this paper is maximum value of the pitch in each vowel segment, amplitude power of the corresponding frame, maximum value of the amplitude energy in each vowel segment, pitch of the corresponding frame, duration of each vowel segment and mean value and rate of change of the first three formants. For these parameters, the ratio between the emotional and neutral speech was used as the recognition characters.
In addition to the above features, LPCC, PLP, MFCC are also taken into consideration for precise decision. Figure 2 is the module for feature extraction.


 Speech Signal Endpoint Detection Preprocessing Adding window, Frame
LPC
LPCC
Figure. 2 the module for feature extraction
4. Speech Emotion Recognition based on GMM
GMM can be described as follow:
li ={ai,mi,Si}, (1)
         MM
p(x|l)=Âab(x),Âa =1 , (2) iii
rr
i=1 i=1
r 1 1rt-1r
bi(x)= D/2 1/2 ×exp{- (x-mi) Si (x-mi)}, (3) (2p) |Si | 2
 where xr is the D-dimensional feature vector, b(xr) (i=1,2,...M)is the density function of i
the member x , p(x | l) is the probability density function of x , and a satisfies: i
M
Âai =1 (i=1,2,...M).
i=1
denoted as:
or
Amplitude, duration, etc.
rrr
The GMM probability function of a speech signal with T frames X = (x , x ,L, x ) can be 12T
T P(X|l)=’p(x |l),
(4)
t t=1
v
MFCC
PLP
 pitch, format, etc.
vvv


T S(X|l)=logP(X|l)=Âlogp(x |l).
According to the statistical characteristic of likelihood probability (LP) output by Gaussian Mixture Model, the likelihood probability with the best model is generally bigger than that of the other GMM, but due to the existence of variations in speech characteristics and noise, some frames’ LP shows a best model that is smaller than that of the others, so the decision may be incorrect. In order to reduce this error recognition rate, some transformation should be introduced to compensate for the likelihood probability, that is, raise the probability with the best model and reduce the probability with the other models. Therefore, a nonlinear compensation transformation is proposed in this paper to solve this problem.
5. Compensation Transformation for GMM
The transformation must satisfy three conditions as follow:
1. The difference of the output probability in different time should be reduced, i.e. increase DS1 ;
T
DS = Â logp(x |l)-logp(x |l)
vv
1 tt1 k t,k=1
t t=1
v
(5)
t1k
2. The difference of the output probability in the same time with different emotion should be increased, i.e. increase DS2 ;
M
DS = Â logp(x |l)-logp(x |l )
vv
2titj i, j=1
i1 j
3. The relative value of the output probability should not be changed.
Assuming that xr is a feature vector, l is the best model corresponded to xr , and l 01
is the other model that is mismatched. If the transformation is linear:
f[p(x |l )]=ap(x |l )+b ti ti
vv
f[p(x|l )]-f[p(x|l)]=a[p(x |l )-p(x |l)], (6) 01t0t1
vvvv
wherea,b=const.Hereset a>0: vvvv
p(x|l )3 p(x|l )¤ f[p(x|l )]3 f[p(x|l )], (7) 0101
p(x|l )£ p(x|l )¤ f[p(x|l )]£ f[p(x|l )]. (8) 0101
From (7) ~ (8), it is obvious that the linear transformation cannot increase or reduce the LP of the output. The compensation could not be linear transformation, so a nonlinear compensation transformation is proposed; the detailed steps are described as follow:
vvvv

1. Compute the probability of the t-th feature vector, where N is the number of the emotions, and T is the number of the frames.
p(x |l)(i=1,2,...N),(t=1,2,...T) ti
2.Normalizep(x |l). ti
v
r p(x |l)
P(x |l)= t tiv
v
v
i maxp(xt |li)
(9)
, (10) where n=2~5, b>1 and b isalwayssetcloseto1.
 3. Compute the output LP. v
v [P(x |l)]n
ti tivn
S(x ,l ) =
4. Introduce the compensation: compute the average probability with K former frames.
 [P(xt |li)] +b vvv1v
K
S(x x ...,x l) = ÂS(x |l) (11)
t-K+1, t-K+2,, t, i K k=1 t+k, i
In general, K also has an influence on output probability, here set K = 2 ~ 5 .
S(x x t-K+1, t-K+2,
..., x l ) as the compensation for S(x | l ) .
5. Take rrrrrr
vvvr
t, i
S'(x |l)=S(x |l)+ad[S(x ,x ,...,x,l)-S(x|l)],
t ti tititit-K+1t-K+2titi
i
whereatiŒ[0,1), ti Ì
ÔÓ -1
.
6. Calculate the joint probability for each model. T
S(X,l)=ÂlogS'(x |l) iti
,
Ï1 S(x ,x ,...,x,l)>S(x |l)
Ô
d= t-K+1t-K+2 ti ti
otherwise
rrrr
(13) 7. Make the decision of which emotion X belongs to. IfS(X,lj)=maxS(X,li), then
t=1 X belongs to l j .
v
i
Assuming two emotions: l , l and two vectors: x , x .Set T = 2 . The output
01 12 probability without transformation:
S(x ,l )=lnp(x |l )+ln[p(x |l )], (14) 01020
vvv
S(x,l)=lnp(x |l)+ln[p(x |l)]. (15) 11121
vvv
lnP(x |l )+lnP(x |l )> lnP(x |l )+lnP(x |l ) 10201121
vvvv
fiP(x |l )P(x |l )-P(x |l)P(x |l)>0 1020 1121
vvvv
fiP(x |l )nP(x |l )n-P(x |l)nP(x |l)n>0, (16) 10201121
vvvv
rr
(12)


When S(x,l ) > S(x,l ) , xr (i=1, 2) belongs to l , otherwise belongs to l . The output 01i01
vv
probability with transformation:
S(x |l )= log( 1 0
P'(x |l )n +b 1,0 1,0 P'(x |l )n +b 10 10
-
-S(0,l )]), (17) 0
-
11 21 without transformation.
S(X |l )-S(X |l ) =log( 0 1
P'(x |l )n P'(x |l )n 1 0 +d a [ 1 0
-
-S(0,l )]) 0
-
P'(x |l )n P'(x |l )n 1 0 +d a [ 1 0
vv
  S(x2 |l0)= log( vv
2 0 +d2,0a2,0[ 2 0 vv
vv
P'(x |l )n P'(x |l )n
vv
-S(1,l0)]). (18) S(x ,l ) and S(x ,l ) are similar to (17)~(18). The decision rule is the same as the one
  P'(x |l)n+b vv
P'(x |l)n+b
20 20
P'(x |l )n +b 1,0 1,0 P'(x |l )n +b 10 10
  vv
P'(x |l )n P'(x |l )n
vv
+log( 2 0 +d2,0a2,0[ 2 0 -S(1,l0)]) vv
  P'(x |l)n+b P'(x |l)n+b 20 20
-
P'(x |l)n P'(x |l)n
vv
1 1 +d a [ 1 vv
1
-S(0,l)]) 1
-log(
-log( 21 +da[ 21 -S(1,l)]), (19)
P'(x |l)n+b 1,1 1,1 P'(x |l)n+b 11 11
  P'(x |l)n P'(x |l)n vv
n r [P(x | l )]
vv
P'(x |l)n+b 2,1 2,1 P'(x |l)n+b 21 21
1
-
-S(t,l). i
-
  Seta=a=a=a=const=a,p=pxl ,S=
10 20 11 22 ti (t i) ti v
n
t+1 i 1. p10 = p20 = 1 , (16) and (19) can be changed into (20) ~ (21):
P P <1 11 21
S(X|l)-S(X|l)=log[
È -logÍ
(20)
0
1
1 (1+b)2
+
--
ad1,0 S00+ad2,0 S10 1+b
2 -
+ad d S00S10]
v
t+1 i [P(x |l)]+b
 1,0 2,0
  ad21S11 ad11S11 + -
p11 p21
ÍÎ(p11 +b)(p21 +b) (p11 +b) (p21 +b)
2
ad d S S ]>0 (21)
11 21 11 21
   1 pp ÊdS+adSdSdSˆ
- 11 21 +aÁ 10 00
(p +b)(p +b) Á 1+b
20 10 - 21 11 - 11 11  ̃ (p +b) (p +b) ̃
    2
(1+b) 11 21 Ë
11 21  ̄
s.t.
(1+b)2 1+b
1 + ad10S00 + ad20S10 + a2d10d20S10S00 > 0
+a2(d d S S -d d S S )>0 (22) 10 20 10 00 11 21 11 21
  
p11 p21 ad21S11 ad11S11 2
+ - +addSS>0
11 21 11 21
a is small enough to ignore the influence of the second and the third item in (22).
(p11 +b)(p21 +b) (p11 +b) (p21 +b)
1 p11 p21 b2 (1- p11 p21) + bp11(1- p21) + bp21(1- p11)
Compared to (20), it can be seen that the LP with transformation is increased. 2. p10 = p21 = 1 , (16) and (19) can be changed into
p20 - p11 > 0 (24)
   where
-= >0(23) (1+b)2 (p11 +b)(p21 +b) (1+b)2(p11 +b)(p21 +b)
   ÈpadSadSp  ̆ S(X|l)-S(X|l)=logÍ 20 + 2010+ 100020+a2ddSS
(1+b)(p +b) 1+b p +b 10 20 10 00 ̇ Í20 20  ̊ Î
0
-- --
+a2(d d S00S10-d d S01S11)>0 (26) 1,0 2,0 1,1 2,1
1
   È
-logÍ 11 + 11 01 + 21 11 11 +a2d d S S ]>0 (25)
p adSadSp
(1+b)(p +b) 1+b p +b 11 21 11 21
   ÍÎ11 11
p11 Êd20S10 ad10S00p20 d11S01 d21S11p11 ˆ -+aÁ+-- ̃
p20
(1+b)(p +b) (1+b)(p +b) Á 1+b p +b 1+b
p +b  ̃ 20 11 Ë 20 11  ̄
      s.t.
(1+b)(p20 +b) 1+b p20 +b
p adSadSp
20 + 20 10 + 10 00 20 +a2d d S S >0
10 20 10 00
   p adSadSp
11 + 1101+ 211111+a2ddSS]>0
   (1+b)(p +b) 1+b p +b 11 11
The first and the second item in (26)
b (p20 - p11). (1+b)(p11 +b)(p20 +b)
11 21 11 21
(27) Compared to (24), (27) has little effect in increasing or reducing probability, except according
 to the convention: IfP(x |l )>P(x |l ), then P(x |l )<P(x |l ). Sod =1,d 10 20 11 21 20 21
=-1, the first and third items in (26) are positive, the second item is far smaller than the first one. Even if the second and the fourth items were negative, the output probability with the best
vvvv
modal would still be bigger than the one with other modals. S10 is always bigger than S01 , and a is small enough to ignore the fourth item. When the LP of xr with l and LP of
xr with l is big, the compensation transformation can enlarge the distance between these two probabilities.
3. p11 = p20 = 1 , the analysis is similar to Derivation 2.
6. Experiment Results
In this paper, six people (three male and three female) have taken part in a recording test. They read 27 sentences using five kinds of emotion (happiness, angry, neutral, surprise and sadness), every sentence was read three times, and 2430 sentences were taken as the experiment materials.
GMM with compensation and GMM without compensation are compared first. In the first experiment, globe features and structural features of the time series were utilized. The result is shown in Table 2. In the second experiment, 12 LPCC, 12 MFCC, 16 PLP were utilized. The result is listed in Table 3. Set K = n = 3, aii o const = 0.01
Table 2. the result of the experiments between compensated and uncompensated emotion recognition (globe features and structural features %)
   Emotion Anger Sadness Happiness Surprise Neutral
Uncompensated GMM 77.6
84.5
73.4
75.8
71.6
Compensated GMM 86.2
99.8
80.0
79.3
77.1
                  Table 3. the result of the experiments between compensated and uncompensated emotion recognition (LPCC, MFCC, PLP %)
   Emotion Anger Sadness Happiness Surprise Neutral
Uncompensated GMM 76.3
82.1
79.6
77.8
80.4
Compensated GMM 84.2
97.8
88.3
82.1
87.0
                  The experiments indicate that the compensation transformation can improve the recognition rate effectively. Angry recognition rate increased 8.2%, sadness recognition rate increased 15.5%, and happiness recognition rate increased 8.5%, surprise recognition rate increased 4%, and neutral recognition rate increased 6%. The selection of K,n,ati also can improve recognition rate. Here, the authors only selected a set of parameters to explain the effectiveness and robustness of the method. Due to the compensation for GMM, the probability of the output has been stabilized and DS2 has been increased.
Table 4 shows another experiment which compared three methods: KNN, NN [7] and compensated GMM (CGMM).


Table 4. KNN, NN, Compensated GMM (%)
    Emotion Anger Sadness Happiness Surprise Neutral
KNN 76.0 82.3 70.5 72.2 78.9
NN CGMM 82.3 86.2 86.0 99.8 71.4 80.0 64.0 79.3 70.6 77.1
                        Compared to KNN, the recognition rate increased 17.5%, happiness increased 7.5%, and surprise increased 7.1%, while neutral decreased 1.7%. This decrease doesn’t effect the improvement of the whole recognition rate. Compared to NN, the average recognition rate also has been increased about 9.7% using CGMM. The results indicate that CGMM also can improve some other methods to a certain degree.
7. Conclusion and Future Works
In this paper, a method based on GMM with compensation transformation is proposed. In speech emotion recognition, the variations in speech characteristics and noise always influence the recognition results. The common method to solve this problem is conventional preprocessing. As the method in this paper deals with this problem in the recognition stage, the likelihood probability of the output with different models has been increased or decreased to reduce these influences. According to a simple analysis, this compensation transformation can reduce this impact effectively, and the examination results also proved it has better emotion recognition rates. However, the recognition rate of happiness and surprise is still not ideal, and the test materials are too few to further experiments. In further research, the authors will extend the experiment sentences first, then do some studies, such as adding more types of noise and the consideration of gender.
